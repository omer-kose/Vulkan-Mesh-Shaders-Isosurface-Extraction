#version 450
#extension GL_GOOGLE_include_directive : enable

#include "input_structures.h"

layout(set = 1, binding = 0) uniform sampler2D depthPyramid;

layout(local_size_x = 64, local_size_x = 1, local_size_z = 1) in;

bool occlusionTest(vec3 lowerCornerPos, vec3 upperCornerPos)
{
	vec3 center = (lowerCornerPos + upperCornerPos) / 2.0f;
	center = (sceneData.view * vec4(center, 1)).xyz;
	float radius = length(upperCornerPos - lowerCornerPos) / 2.0f; 
	vec4 aabb;
	bool visible = false;
	if (projectBox(lowerCornerPos, upperCornerPos, zNear, sceneData.viewproj, aabb))
	{
		float width = (aabb.z - aabb.x) * depthPyramidWidth;
		float height = (aabb.w - aabb.y) * depthPyramidHeight;

		float level = floor(log2(max(width, height)));

		// Sampler is set up to do min reduction, so this computes the minimum depth of a 2x2 texel quad
		float sampledDepth = textureLod(depthPyramid, (aabb.xy + aabb.zw) * 0.5, level).x;
		float currentDepth = zNear / (-center.z - radius);
		visible = currentDepth > sampledDepth;
	}

	return visible;
}

// TODO: Precompute and store frustum planes
bool frustumCullTest(vec3 bmin, vec3 bmax)
{
    // Test all 8 corners against clip space [-w, w]
    // Reject if all are outside the same plane
    vec4 corners[8] = {
        sceneData.viewproj * vec4(bmin.x, bmin.y, bmin.z, 1.0),
        sceneData.viewproj * vec4(bmax.x, bmin.y, bmin.z, 1.0),
        sceneData.viewproj * vec4(bmin.x, bmax.y, bmin.z, 1.0),
        sceneData.viewproj * vec4(bmax.x, bmax.y, bmin.z, 1.0),
        sceneData.viewproj * vec4(bmin.x, bmin.y, bmax.z, 1.0),
        sceneData.viewproj * vec4(bmax.x, bmin.y, bmax.z, 1.0),
        sceneData.viewproj * vec4(bmin.x, bmax.y, bmax.z, 1.0),
        sceneData.viewproj * vec4(bmax.x, bmax.y, bmax.z, 1.0)
    };

    for(int plane = 0; plane < 6; ++plane) 
    {
        int outsideCount = 0;
        for (int i = 0; i < 8; ++i) 
        {
            vec4 p = corners[i];
            float w = p.w;
            float v;
            if (plane == 0) v =  p.x + w; // left   (x >= -w)
            if (plane == 1) v = -p.x + w; // right  (x <= w)
            if (plane == 2) v =  p.y + w; // bottom (y >= -w)
            if (plane == 3) v = -p.y + w; // top    (y <= w)
            if (plane == 4) v =  p.z + w; // near   (z >= -w in GL)
            if (plane == 5) v = -p.z + w; // far    (z <= w in GL)
            if (v < 0) outsideCount++;
        }
        if (outsideCount == 8) return false; // completely outside
    }
    return true;
}

bool cullTest(vec3 lowerCornerPos, vec3 upperCornerPos)
{
    if(frustumCullTest(lowerCornerPos, upperCornerPos))
    {
        if(occlusionTest(lowerCornerPos, upperCornerPos))
        {
            return true;
        }
    }

    return false;
}

void main()
{
    // Each thread works on a chunk
    uint id = gl_GlobalInvocationID.x;
    
    if(id >= numActiveChunks)
    {
        return;
    }

    uint chunkID = activeChunkIndicesBuffer.activeChunkIndices[id];
    ChunkMetadata metadata = chunkMetadataBuffer.chunkMetadata[chunkID];

    if(cullTest(metadata.lowerCornerPos, metadata.upperCornerPos))
    {
        uint numTaskGroups = (mcSettings.gridSize.x * mcSettings.gridSize.y * mcSettings.gridSize.z) / (BLOCK_SIZE * BLOCK_SIZE * BLOCK_SIZE); // Grid size (chunk size) will always be a multiple of 2 and bigger than BLOCK_SIZE, so no need for ceil  
        uint gid = atomicAdd(drawChunkCountBuffer.drawChunkCount, numTaskGroups);
        
        for(uint i = 0; i < numTaskGroups; ++i)
        {
            chunkDrawDataBuffer.chunkDrawData[gid + i].chunkID = chunkID;
            chunkDrawDataBuffer.chunkDrawData[gid + i].localWorkgroupID = i;
        }  
    }

    // Block version but not working properly. Still dispatching many unnecessary blocks. In the next version, I can drop off blocks from processing in the task shader when I make task shaders process a chunk fully instead of dispatching a group per block. It is the bottleneck
    /*
    if(frustumCullTest(metadata.lowerCornerPos, metadata.upperCornerPos))
    {
        if(occlusionTest(metadata.lowerCornerPos, metadata.upperCornerPos))
        {
            
            uint numTaskGroups = (mcSettings.gridSize.x * mcSettings.gridSize.y * mcSettings.gridSize.z) / (BLOCK_SIZE * BLOCK_SIZE * BLOCK_SIZE); // Grid size (chunk size) will always be a multiple of 2 and bigger than BLOCK_SIZE, so no need for ceil  
            uint gid = atomicAdd(drawChunkCountBuffer.drawChunkCount, numTaskGroups);
            
            for(uint i = 0; i < numTaskGroups; ++i)
            {
                chunkDrawDataBuffer.chunkDrawData[gid + i].chunkID = chunkID;
                chunkDrawDataBuffer.chunkDrawData[gid + i].localWorkgroupID = i;
            }
            
            
            
                uvec3 numTaskGroups = mcSettings.gridSize / BLOCK_SIZE;
                uint gid;
                uint i = 0;
                vec3 step = (metadata.upperCornerPos - metadata.lowerCornerPos) / mcSettings.gridSize;
                for(uint z = 0; z < numTaskGroups.z; ++z)
                {
                    for(uint y = 0; y < numTaskGroups.y; ++y)
                    {
                        for(uint x = 0; x < numTaskGroups.x; ++x)
                        {
                            vec3 blockLowerPos = metadata.lowerCornerPos + vec3(x, y, z) * BLOCK_SIZE * step;
                            vec3 blockUpperPos = blockLowerPos + vec3((BLOCK_SIZE - 1) * step);
                            if(occlusionTest(blockLowerPos, blockUpperPos))
                            {
                                gid = atomicAdd(drawChunkCountBuffer.drawChunkCount, 1);
                                uint i = x + numTaskGroups.x * (y + numTaskGroups.y * z);
                                chunkDrawDataBuffer.chunkDrawData[gid].chunkID = chunkID;
                                chunkDrawDataBuffer.chunkDrawData[gid].localWorkgroupID = i;
                            }
                        }
                    }
                }
            
        }
    }
    */
    
}